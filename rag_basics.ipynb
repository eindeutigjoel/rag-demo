{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval augmented generation using Elasticsearch and Llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates:\n",
    "- Transform mimacom's handbook into a vector dataset and index into Elasticsearch.\n",
    "- Embed a question using `SentenceTransformer`.\n",
    "- Perform a vector search aka k-NN search and retrieve relevant documents.\n",
    "- Pass top results to LLama 3 using __[Ollama](http://ollama.com)__ for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages\n",
    "!python3 -m pip install -qU PyPDF2 elasticsearch sentence-transformers ollama\n",
    "\n",
    "# import modules\n",
    "import PyPDF2, re, os\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Elastic\n",
    "An elastic instance should be running. As mentioned before, a docker-compose file for an elasticsearch container is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': '7917c3e5768b', 'cluster_name': 'docker-cluster', 'cluster_uuid': 'wFoUA_1xQdaT0elbUn8r1Q', 'version': {'number': '8.8.0', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': 'c01029875a091076ed42cdb3a41c10b1a9a5a20f', 'build_date': '2023-05-23T17:16:07.179039820Z', 'build_snapshot': False, 'lucene_version': '9.6.0', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "# Replace if need it.\n",
    "ES_HOST = \"http://localhost:9200\"\n",
    "ES_INDEX = \"ragdemo-1\"\n",
    "\n",
    "client = Elasticsearch(ES_HOST)\n",
    "\n",
    "# Check if the elastic instance is running.\n",
    "print(client.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create index mapping\n",
    "Now it is time to create an index mapping. \n",
    "Elastic uses `dense_vector` as data type to store a numeric representation of the semantic or meaning of a piece of data.\n",
    "\n",
    "We need to target `dense_vector` field in order to perform kNN search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define index mapping\n",
    "mappings = {\n",
    "    \"properties\": {\n",
    "        \"content\": {\n",
    "            \"type\": \"text\"\n",
    "        },\n",
    "        \"embedding\": {\n",
    "            \"type\": \"dense_vector\",\n",
    "            \"dims\": 384,\n",
    "            \"index\": True, # Default value\n",
    "            \"similarity\": \"cosine\" # Default value\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create an index if it exist\n",
    "try:\n",
    "    client.indices.create(index = ES_INDEX, mappings = mappings)\n",
    "except Exception as e:\n",
    "    print(\"Create index ignored\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data from PDF\n",
    "For this example we use my company's handbook as data. The data is in a PDF and one page represents one document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "pdfFile = open(os.path.abspath(home) + '/company_book.pdf', 'rb')\n",
    "pdfReader = PyPDF2.PdfReader(pdfFile)\n",
    "\n",
    "# Cleanup data. Remove unnecessary text.\n",
    "pattern = r'page \\d+ of \\d+ Internal Use'\n",
    "toIndex = []\n",
    "for page in pdfReader.pages:\n",
    "    text = page.extract_text()\n",
    "    cleanedText = re.sub(pattern, '', text)\n",
    "    cleanedText = cleanedText.replace(\"\\n\", \"\")\n",
    "    toIndex.append((cleanedText))\n",
    "pdfFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup encoders\n",
    "In this demo we are using `all-MiniLM-L6-v2` model. This encoder embeds text in 384 dimensions. It means that the model is describing the text using 384 different features or aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/demorag/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Setup encoder and make sure it uses GPU on Mac Silicon\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the extracted data to ES\n",
    "for i, t in enumerate(toIndex):\n",
    "    embeddedContent = model.encode(t, device = device)\n",
    "    document = {\n",
    "        'content': t,\n",
    "        'embedding': embeddedContent  \n",
    "    }\n",
    "    response = client.index(index = ES_INDEX, body = document, id = i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define knn query and retrieve relevant documents\n",
    "\n",
    "We define a function that builds an ES knn query, perform the search and return relevant documents.\n",
    "As mentioned before, we need to target the field that contains the vector data, in our case it's called `embedding`. Also we pass the question or instruction from the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareAndDoSearch(embeddedQuery):\n",
    "    response = client.search(\n",
    "        index = ES_INDEX,\n",
    "        knn = {\n",
    "            'field': 'embedding',\n",
    "            'query_vector': embeddedQuery,\n",
    "            'num_candidates': 500,\n",
    "            'k': 10,\n",
    "        },\n",
    "        size = 3\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    for hit in response['hits']['hits']:\n",
    "        documents.append(re.escape(hit['_source']['content']))\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write and encode the prompt\n",
    "Since the search is targeting a `dense_vector` field, we must encode our question/instruction using the same encoder that we used before.\n",
    "\n",
    "For building the final prompt we attached the text of relevant documents for context, with the actual question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of retrieved documents:3\n"
     ]
    }
   ],
   "source": [
    "# Encode the query\n",
    "query = \"Tell me if they provide employees with corporate cars\"\n",
    "embeddedQuery = model.encode(query, device = device)\n",
    "\n",
    "# Make a prompt based on context and question\n",
    "documents = prepareAndDoSearch(embeddedQuery)\n",
    "print(\"Amount of retrieved documents:\" + str(len(documents)))\n",
    "\n",
    "prompt = \"Context: \" + ''.join(documents) + \"  answer the following question briefly: \" + query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let LLAMA shine \n",
    "`ollama` serves as a powerful and user-friendly platform for running LLMs on your local machine. And through ollama we are invoking `llama3.2` \n",
    "\n",
    "*Note* \n",
    "As you can see there is an option `num_ctx`. It means that we can give `llama` max. 3200 tokens to process.\n",
    "By default this value is set to 2000 tokens and the maximum llama can handel is 128000 tokens. This can be seen as a limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "output = ollama.generate(\n",
    "  model = \"llama3.1\",\n",
    "  prompt = prompt,\n",
    "  options = {\n",
    "    \"num_ctx\": 32000\n",
    "  }\n",
    ")\n",
    "\n",
    "print(output['response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demorag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
